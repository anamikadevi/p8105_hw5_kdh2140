---
title: "Homework 5: Iteration"
author: "Kristina Howell"
output: github_document
---

```{r settings, message = FALSE}
# These settings will be used throughout the document.

library(tidyverse)
library(rvest)
library(httr)
library(patchwork)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis", 
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Problem 1

#### Import the dataset

```{r import_1, message=FALSE, warning=FALSE}
homicide_df = 
  read_csv("homicide-data.txt") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

#### Data description

This dataset was collected by the Washington post with a wide range of demographic information about each homicide, the victim, and the outcome of the investigation. It was obtained from the purposes of this report from a publicly available github repository. 

#### Summary Statistics 

```{r tidy_1, message=FALSE, warning=FALSE}
aggregate_df = 
homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(), 
    hom_unsolved = sum(resolved == "unsolved")
  )
```

#### Prop testing

```{r prop_1}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

#### Iteration

```{r iter_1}
results_df = 
  aggregate_df %>% 
  mutate(
    
    prop_tests = map2(.x = hom_unsolved, 
                      .y = hom_total, 
                      ~prop.test(x = .x, n = .y)), 
    
    tidy_tests = map(.x = prop_tests, 
                     ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

#### Data Exploration

```{r eda_1}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
           geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


## Problem 2

#### Data tidying

```{r import_2, message=FALSE, warning=FALSE}
path_df = 
  tibble(path = list.files("./data")) %>% 
  mutate(path = str_c("data/", path),
         data = map(path, read_csv)) %>% 
  unnest(data) %>% 
  separate(path, into = c("x", "study_arm"), sep = 5) %>% 
  separate(study_arm, into = c("study_arm", "subject_id"), sep = "_") %>% 
  separate(subject_id, into = c("subject_id", "y"), sep = 2) %>% 
  select(-x, -y) %>% 
  pivot_longer(week_1:week_8, 
               names_to = "week",
               names_prefix = "week_",
               values_to = "observation")

head(path_df)
```

The importing and data tidying process begins by creating a tibble of the file names and adding the relative path to each file name. The data is umported using the map function to read csv for each entry. The data is then unnested and the file name is separated into useful columns (study_arm, subject_id). Lastly, the data is pivoted longer to create week and observation columns. The first few observations are displayed for reference. 

#### Creating a spaghetti plot

```{r plot_2}
path_df %>% 
  group_by(study_arm) %>% 
  ggplot(aes(x = week, y = observation, color = subject_id, group = subject_id)) +
  geom_line() + 
  facet_grid(. ~ study_arm) +
  labs(
    title = "Observations over time by arm"
  )
```

The observations for both the control and experimental arm of the study begin around 0 - 2.5, with the experimental arm having a slightly larger range. The control has some variance over the eight weeks, but maintains and ends roughly around the same range as it began, 0 - 2.5. The experimental group drastically increases over the 8 weeks, with some variance on the individual level, but ends with a higher overall range of 2.5 - 7.5. 

## Problem 3

#### Function to simulate data and perform t-test

```{r func_3}
sim_t_test = function(mu) {
  
  sim_data =   
    tibble(
      x = rnorm(n = 30, mean = mu, sd = 5))
  
  sim_data %>% 
  t.test(y = NULL, 
         conf.level = 0.95) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
  
}
```

The above function creates a tibble generated from a normal distribution of size 30 and standard deviation 5. It then uses that data to perform a t-test at alpha = 0.05. Only the estimate and p-value of the t-test are retained. 

#### Simulate data

```{r sim_3, cache = TRUE}

sim_results = 
  
  tibble(
    mu = c(0, 1, 2, 3, 4, 5, 6)) %>% 
  
  mutate(
      output_lists = map(.x = mu, ~ rerun(5000, sim_t_test(.x))), 
      estimate_df = map(output_lists, bind_rows)) %>% 
  
  select(-output_lists) %>% 
  unnest(estimate_df)
```

The above code runs a simulation of the function previously created, sim_t_test, with various values of mu, 5000 times over. It then binds the rows of the output and unnests the data. 

#### Plots

##### Plot #1: Proportion: Null Hypothesis Not Rejected / Hypothesis Rejected 

```{r plot_3a, message=FALSE, warning=FALSE}
sim_results %>% 
  group_by(mu) %>% 
   mutate(
     results = case_when(
      p.value < 0.05 ~ "rejected",
      p.value >= 0.05 ~ "not_rejected"), 
     results = as.factor(results)) %>% 
  summarize(proportion = sum(results == "not_rejected") / sum(results == "rejected")) %>% 
  ggplot(aes(x = mu, y = proportion)) +
  geom_point() +
  labs(
          title = "Proportion: Null Hypothesis Not Rejected / Hypothesis Rejected ",
          x = "True Value of mu",
          y = "Hypothesis outcome proportion",
          caption = "Rejected over not rejected provides values of infinity for mu = 5, 6"
          )
```

The majority of hypothesis tests where the null hypothesis was not rejected lie between mu = 0 and mu = 2, as the true outcome would expect for the null hypothesis to not be rejected for mu = 0 and random sampling distribution would also allow for some estimated values of mu = 0 for true mu = 1 or 2. 

##### Plots #2 & #3: Average Estimate Values of mu

```{r plot_3b, message=FALSE, warning=FALSE}
all = sim_results %>% 
        group_by(mu) %>% 
        summarize(
          average_mu = mean(estimate)) %>% 
        ggplot(aes(x = mu, y = average_mu)) +
        geom_point() +
          labs(
          title = "Average Estimate of mu, all values",
          x = "True Value of mu",
          y = "Average Estimate of mu"
          )

some = sim_results %>% 
        filter(p.value < 0.05) %>% 
        group_by(mu) %>% 
        summarize(
          average_mu = mean(estimate)) %>% 
        ggplot(aes(x = mu, y = average_mu)) +
        geom_point() +
          labs(
          title = "Average Estimate of mu, H0 rejected",
          x = "True Value of mu",
          y = "Average Estimate of mu"
          )

all / some
```

The above set of plots represent the average estimated value of mu by true mu for the sim_results dataset. The **first plot** includes all estimated values of mu, significant or not at alpha = 0.05. The **second plot** only includes the estimated values of mu that are significant at an alpha = 0.05. 

The sample average of mu across tests for which the null is rejected is not approximately equal to the true value of mu, as it only includes observations that  are different enough from 0 to elicit a significance level larger than 0.05. For samples in which the true mean is closer to 0 (i.e. 1, 2), there are more samples that are not rejected, which heightens the average estimate of mu for a filtered dataset of only values of h0 that are rejected. The sample average of mu across tests is equal to the true value of mu in the first plot, which includes all values and allows for the a true calculation of mu derived from all samples. 







